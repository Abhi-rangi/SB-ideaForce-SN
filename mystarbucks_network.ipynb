{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import community              as community_louvain     # python‑louvain\n",
    "from networkx.algorithms.community import quality      # modularity\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUG_FILE   = \"top100_suggestions.csv\"\n",
    "COM_FILE   =  \"top100_comments.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_id = {\"suggestionId\": int, \"commentId\": int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSVs...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CSVs...\")\n",
    "sug = pd.read_csv(SUG_FILE)\n",
    "com = pd.read_csv(COM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building edge lists...\n"
     ]
    }
   ],
   "source": [
    "# ===== 2. Build edge lists =================================================\n",
    "print(\"Building edge lists...\")\n",
    "\n",
    "# 2a. Comment‑flow  (commenter -> suggestion author)\n",
    "edge_flow = (\n",
    "    com.merge(\n",
    "        sug[[\"suggestionId\", \"author\"]],\n",
    "        on=\"suggestionId\",\n",
    "        suffixes=(\"_com\", \"_sug\"),\n",
    "    )\n",
    "    .groupby([\"author_com\", \"author_sug\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"weight\")\n",
    "    .query(\"author_com != author_sug\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Co‑commenter (users who commented on the same suggestion)\n",
    "pairs = (\n",
    "    com[[\"suggestionId\", \"author\"]]\n",
    "    .drop_duplicates()\n",
    "    .merge(com[[\"suggestionId\", \"author\"]], on=\"suggestionId\")\n",
    "    .query(\"author_x < author_y\")                        # remove self & symmetric dupes\n",
    "    .value_counts([\"author_x\", \"author_y\"])\n",
    "    .reset_index(name=\"weight\")\n",
    "    .rename(columns={\"author_x\": \"u\", \"author_y\": \"v\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c. User‑Idea bipartite (author OR commenter edges)\n",
    "ui_edges = pd.concat(\n",
    "    [\n",
    "        sug[[\"author\", \"suggestionId\"]].rename(\n",
    "            columns={\"author\": \"user\", \"suggestionId\": \"idea\"}\n",
    "        ),\n",
    "        com[[\"author\", \"suggestionId\"]].rename(\n",
    "            columns={\"author\": \"user\", \"suggestionId\": \"idea\"}\n",
    "        ),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d. Suggestion projection (ideas linked via shared users)\n",
    "sugg_pairs = (\n",
    "    ui_edges.merge(ui_edges, on=\"user\")\n",
    "    .query(\"idea_x < idea_y\")\n",
    "    .value_counts([\"idea_x\", \"idea_y\"])\n",
    "    .reset_index(name=\"weight\")\n",
    "    .rename(columns={\"idea_x\": \"s1\", \"idea_y\": \"s2\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing NetworkX objects…\n",
      "Tagging roles…\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Build NetworkX graphs\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "print(\"Constructing NetworkX objects…\")\n",
    "\n",
    "G_flow = nx.DiGraph()\n",
    "G_flow.add_weighted_edges_from(\n",
    "    edge_flow[[\"author_com\", \"author_sug\", \"weight\"]].values\n",
    ")\n",
    "\n",
    "G_co = nx.Graph()\n",
    "G_co.add_weighted_edges_from(pairs[[\"u\", \"v\", \"weight\"]].values)\n",
    "\n",
    "G_bip = nx.Graph()\n",
    "G_bip.add_nodes_from(ui_edges[\"user\"].unique(), bipartite=\"user\")\n",
    "G_bip.add_nodes_from(ui_edges[\"idea\"].unique(), bipartite=\"idea\")\n",
    "G_bip.add_edges_from(ui_edges[[\"user\", \"idea\"]].values)\n",
    "\n",
    "G_proj = nx.Graph()\n",
    "G_proj.add_weighted_edges_from(sugg_pairs[[\"s1\", \"s2\", \"weight\"]].values)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4b.  Role tagging  (pure‑heuristic, optional override file)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "print(\"Tagging roles…\")\n",
    "\n",
    "# -- 1. Build per‑user activity metrics ---------------------------------------\n",
    "comment_cnt = com[\"author\"].value_counts()\n",
    "suggest_cnt = sug[\"author\"].value_counts()\n",
    "vote_totals = sug.groupby(\"author\")[\"votes\"].sum()\n",
    "\n",
    "user_metrics = (\n",
    "    pd.DataFrame({\"comments\": comment_cnt,\n",
    "                  \"suggestions\": suggest_cnt,\n",
    "                  \"votes\": vote_totals})\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "# -- 2. Thresholds for “contributor” status -----------------------------------\n",
    "activity_cut = user_metrics[\"comments\"].quantile(0.90)   # top 10 % by comments\n",
    "vote_cut     = user_metrics[\"votes\"].quantile(0.95)      # or top 5 % by total votes\n",
    "\n",
    "def infer_role(user: str, row) -> str:\n",
    "    \"\"\"Heuristic mapping -> expert / contributor / client.\"\"\"\n",
    "    uname = str(user).lower()\n",
    "    if uname.startswith((\"sbx\", \"starbucks_\")):           # employee / expert flag\n",
    "        return \"expert\"\n",
    "    if (row[\"comments\"] >= activity_cut) or (row[\"votes\"] >= vote_cut):\n",
    "        return \"contributor\"\n",
    "    return \"client\"\n",
    "\n",
    "role_dict = {\n",
    "    user: infer_role(user, row) for user, row in user_metrics.iterrows()\n",
    "}\n",
    "\n",
    "# -- 3. Optional override via external CSV ------------------------------------\n",
    "# If someday you receive an authoritative mapping file:\n",
    "# if ROLE_MAP.exists():\n",
    "#     print(\"Override: applying roles from\", ROLE_MAP.name)\n",
    "#     overrides = pd.read_csv(ROLE_MAP)        # columns: user, role\n",
    "#     role_dict.update(dict(zip(overrides[\"user\"], overrides[\"role\"])))\n",
    "\n",
    "# -- 4. Attach roles to graph nodes -------------------------------------------\n",
    "# Any node missing from user_metrics (edge‑case) defaults to 'client'\n",
    "for u in G_co.nodes():\n",
    "    G_co.nodes[u][\"role\"] = role_dict.get(u, \"client\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Louvain…\n",
      "Calculating modularity…\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Community detection & quality\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "print(\"Running Louvain…\")\n",
    "part_co   = community_louvain.best_partition(G_co,   weight=\"weight\")\n",
    "part_proj = community_louvain.best_partition(G_proj, weight=\"weight\")\n",
    "\n",
    "nx.set_node_attributes(G_co,   part_co,   \"community\")\n",
    "nx.set_node_attributes(G_proj, part_proj, \"community\")\n",
    "\n",
    "print(\"Calculating modularity…\")\n",
    "Q_co   = quality.modularity(G_co,   [ {n for n,c in part_co.items()   if c==k} for k in set(part_co.values()) ],   weight=\"weight\")\n",
    "Q_proj = quality.modularity(G_proj, [ {n for n,c in part_proj.items() if c==k} for k in set(part_proj.values()) ], weight=\"weight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating analytics…\n",
      "Computing centralities…\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# 6.  Analytics tables\n",
    "# --------------------------------------------------------------------------- #\n",
    "print(\"Generating analytics…\")\n",
    "\n",
    "# 6a. Community sizes\n",
    "comm_sizes = (\n",
    "    pd.Series(part_co)\n",
    "      .value_counts()\n",
    "      .rename_axis(\"community\")\n",
    "      .reset_index(name=\"size\")\n",
    "      .sort_values(\"community\")\n",
    ")\n",
    "comm_sizes[\"modularity_G_co\"] = Q_co\n",
    "comm_sizes.to_csv(\"gephi_100_updated/community_sizes_co.csv\", index=False)\n",
    "\n",
    "# 6b. Inter‑community edge weights\n",
    "edges_inter = [\n",
    "    {\"c1\": min(part_co[u], part_co[v]),\n",
    "     \"c2\": max(part_co[u], part_co[v]),\n",
    "     \"weight\": d[\"weight\"]}\n",
    "    for u, v, d in G_co.edges(data=True) if part_co[u] != part_co[v]\n",
    "]\n",
    "pd.DataFrame(edges_inter)\\\n",
    "  .groupby([\"c1\", \"c2\"])[\"weight\"].sum()\\\n",
    "  .reset_index()\\\n",
    "  .to_csv(\"gephi_100_updated/inter_edges_co.csv\", index=False)\n",
    "\n",
    "# 6c. Role mixing matrix & assortativity\n",
    "roles      = nx.get_node_attributes(G_co, \"role\")\n",
    "role_set   = sorted(set(roles.values()))\n",
    "mix_mtx    = pd.DataFrame(0, index=role_set, columns=role_set, dtype=int)\n",
    "for u, v in G_co.edges():\n",
    "    mix_mtx.loc[roles[u], roles[v]] += 1\n",
    "mix_mtx.to_csv(\"gephi_100_updated/role_mixing_co.csv\")\n",
    "\n",
    "assort = nx.attribute_assortativity_coefficient(G_co, \"role\")\n",
    "\n",
    "# 6d. Centrality by role\n",
    "print(\"Computing centralities…\")\n",
    "btw  = nx.betweenness_centrality(G_co, weight=\"weight\")\n",
    "deg  = dict(G_co.degree(weight=\"weight\"))\n",
    "\n",
    "cent_rows = []\n",
    "for u in G_co.nodes():\n",
    "    cent_rows.append({\n",
    "        \"user\": u,\n",
    "        \"role\": roles[u],\n",
    "        \"degree_w\": deg[u],\n",
    "        \"betweenness\": btw[u],\n",
    "        \"community\": part_co[u]\n",
    "    })\n",
    "centrality_df = pd.DataFrame(cent_rows)\n",
    "centrality_df.sort_values([\"role\", \"betweenness\"], ascending=[True, False])\\\n",
    "             .to_csv(\"gephi_100_updated/centrality_by_role.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building idea feature set…\n",
      "Dropping constant predictors: ['Intercept']\n",
      "Fitting logistic regression…\n",
      "  Singular matrix persists. Switching to L2‑regularised (lbfgs) fit…\n",
      "Logit pseudo‑R²: -1.1322161947352445\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7.  Idea‑level feature table & modelling  (robust version)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "print(\"Building idea feature set…\")\n",
    "\n",
    "# ── 7.1  Assemble features ───────────────────────────────────────────────────\n",
    "n_comments = com.groupby(\"suggestionId\").size().rename(\"n_comments\")\n",
    "\n",
    "author_btw  = {u: btw.get(u, 0) for u in sug[\"author\"]}\n",
    "author_deg  = {u: deg.get(u, 0) for u in sug[\"author\"]}\n",
    "author_comm = {u: part_co.get(u, -1) for u in sug[\"author\"]}\n",
    "author_role = {u: roles.get(u, \"unknown\") for u in sug[\"author\"]}\n",
    "\n",
    "idea_df = sug.copy()\n",
    "idea_df = idea_df.merge(n_comments, left_on=\"suggestionId\", right_index=True, how=\"left\")\n",
    "idea_df[\"n_comments\"].fillna(0, inplace=True)\n",
    "\n",
    "idea_df[\"author_betweenness\"] = idea_df[\"author\"].map(author_btw)\n",
    "idea_df[\"author_degree_w\"]    = idea_df[\"author\"].map(author_deg)\n",
    "idea_df[\"author_community\"]   = idea_df[\"author\"].map(author_comm)\n",
    "idea_df[\"author_role\"]        = idea_df[\"author\"].map(author_role)\n",
    "\n",
    "# Success label\n",
    "if \"implemented\" in idea_df.columns:\n",
    "    idea_df[\"success\"] = idea_df[\"implemented\"].astype(int)\n",
    "else:\n",
    "    top_decile = idea_df[\"votes\"].quantile(0.90)\n",
    "    idea_df[\"success\"] = (idea_df[\"votes\"] >= top_decile).astype(int)\n",
    "\n",
    "idea_df.to_csv(\"gephi_100_updated/idea_features.csv\", index=False)\n",
    "\n",
    "# ── 7.2  Prepare design matrix w/ patsy ───────────────────────────────────────\n",
    "import patsy\n",
    "formula = \"success ~ votes + n_comments + author_betweenness + C(category)\"\n",
    "\n",
    "y, X = patsy.dmatrices(formula, data=idea_df, return_type=\"dataframe\")\n",
    "\n",
    "# Drop constant columns (zero variance) to avoid singularity\n",
    "constant_cols = [col for col in X.columns if X[col].nunique() == 1]\n",
    "if constant_cols:\n",
    "    print(\"Dropping constant predictors:\", constant_cols)\n",
    "    X = X.drop(columns=constant_cols)\n",
    "\n",
    "# ── 7.3  Check rank & condition number ───────────────────────────────────────\n",
    "rank = np.linalg.matrix_rank(X.values)\n",
    "if rank < X.shape[1]:\n",
    "    print(f\"Warning: design matrix not full rank ({rank}/{X.shape[1]}). \"\n",
    "          \"Attempting to drop collinear columns…\")\n",
    "    # Simple VIF‑based filter\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    keep = []\n",
    "    for i, col in enumerate(X.columns):\n",
    "        vif = variance_inflation_factor(X.values, i)\n",
    "        if np.isfinite(vif) and vif < 50:      # threshold; adjust as needed\n",
    "            keep.append(col)\n",
    "        else:\n",
    "            print(f\"  dropping {col} (VIF ≈ {vif:.1f})\")\n",
    "    X = X[keep]\n",
    "\n",
    "# ── 7.4  Fit logistic model with graceful back‑off ───────────────────────────\n",
    "print(\"Fitting logistic regression…\")\n",
    "import statsmodels.api as sm\n",
    "\n",
    "try:\n",
    "    logit_model = sm.Logit(y, X).fit(disp=False)\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"  Singular matrix persists. Switching to L2‑regularised (lbfgs) fit…\")\n",
    "    logit_model = sm.Logit(y, X).fit(\n",
    "        disp=False,\n",
    "        method=\"lbfgs\",\n",
    "        maxiter=100,\n",
    "        penalization=\"l2\",\n",
    "        alpha=1.0\n",
    "    )\n",
    "\n",
    "with open(\"gephi_100_updated/logit_summary.txt\", \"w\") as f:\n",
    "    f.write(logit_model.summary().as_text())\n",
    "\n",
    "print(\"Logit pseudo‑R²:\", logit_model.prsquared)\n",
    "\n",
    "# # ── 7.5  (Optional) Cox PH if timestamps present & lifelines available ───────\n",
    "# if HAS_LIFELINES and {\"created_ts\", \"implemented_ts\"}.issubset(idea_df.columns):\n",
    "#     print(\"Fitting Cox model…\")\n",
    "#     idea_df[\"duration\"] = (\n",
    "#         pd.to_datetime(idea_df[\"implemented_ts\"])\n",
    "#       - pd.to_datetime(idea_df[\"created_ts\"])\n",
    "#     ).dt.days\n",
    "#     idea_df = idea_df.dropna(subset=[\"duration\"])\n",
    "#     if len(idea_df) > 30:                                 # need enough rows\n",
    "#         cph = CoxPHFitter()\n",
    "#         cph.fit(\n",
    "#             idea_df[\n",
    "#                 [\"duration\", \"success\", \"votes\", \"n_comments\", \"author_betweenness\"]\n",
    "#             ],\n",
    "#             duration_col=\"duration\",\n",
    "#             event_col=\"success\",\n",
    "#         )\n",
    "#         with open(\"gephi_100_updated/cox_summary.txt\", \"w\") as f:\n",
    "#             f.write(cph.summary.to_string())\n",
    "#     else:\n",
    "#         print(\"  Not enough complete duration records for Cox model.\")\n",
    "# else:\n",
    "#     print(\"Cox model skipped (lifelines not installed or timestamp cols missing).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing GEXF layers…\n",
      "============================================================\n",
      "Pipeline complete.  Key stats\n",
      "- Number of users           : 3075\n",
      "- Number of comments edges  : 3432\n",
      "- Modularity (G_co)         : 0.842\n",
      "- Role assortativity (G_co) : -0.001\n",
      "- Logistic LL / Pseudo‑R²   : -69.31471805599453 / -1.1322161947352445\n",
      "Outputs saved to:/gephi_100_updated\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# 8.  Export graphs\n",
    "# --------------------------------------------------------------------------- #\n",
    "print(\"Writing GEXF layers…\")\n",
    "nx.write_gexf(G_flow, \"gephi_100_updated/comment_flow.gexf\")\n",
    "nx.write_gexf(G_co,   \"gephi_100_updated/co_commenter.gexf\")\n",
    "nx.write_gexf(G_bip,  \"gephi_100_updated/user_idea_bipartite.gexf\")\n",
    "nx.write_gexf(G_proj, \"gephi_100_updated/suggestion_projection.gexf\")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 9.  Final report\n",
    "# --------------------------------------------------------------------------- #\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline complete.  Key stats\")\n",
    "print(\"- Number of users           :\", G_co.number_of_nodes())\n",
    "print(\"- Number of comments edges  :\", G_flow.number_of_edges())\n",
    "print(f\"- Modularity (G_co)         : {Q_co:.3f}\")\n",
    "print(f\"- Role assortativity (G_co) : {assort:.3f}\")\n",
    "print(\"- Logistic LL / Pseudo‑R²   :\", logit_model.llf, \"/\", logit_model.prsquared)\n",
    "print(\"Outputs saved to:\" +\"/gephi_100_updated\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
