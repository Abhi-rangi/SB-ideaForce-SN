{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings, numpy as np, pandas as pd, networkx as nx\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import community as community_louvain\n",
    "from networkx.algorithms.community import quality as nxq\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUG_FILE = \"../combined_500_suggestions.csv\"\n",
    "COM_FILE =  \"../combined_500_comments.csv\"\n",
    "sug = pd.read_csv(SUG_FILE)\n",
    "com = pd.read_csv(COM_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z_/2px0j93s1m5dfxvjqvtkp4tc0000gn/T/ipykernel_95874/1461517252.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
      "/var/folders/z_/2px0j93s1m5dfxvjqvtkp4tc0000gn/T/ipykernel_95874/1461517252.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "# basic tidy\n",
    "for df in (sug, com):\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "sug.dropna(subset=[\"suggestionId\", \"author\"], inplace=True)\n",
    "com.dropna(subset=[\"commentId\", \"suggestionId\", \"author\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building global edge lists…\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2.  GRAPH CONSTRUCTION  (Q1)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"Building global edge lists…\")\n",
    "\n",
    "# 2a  Comment‑flow  (directed, weighted)\n",
    "edge_flow = (\n",
    "    com.merge(sug[[\"suggestionId\", \"author\"]], on=\"suggestionId\",\n",
    "              suffixes=(\"_com\", \"_sug\"))\n",
    "       .groupby([\"author_com\", \"author_sug\"])\n",
    "       .size().reset_index(name=\"weight\")\n",
    "       .query(\"author_com != author_sug\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b  Co‑commenter  (undirected, weighted)\n",
    "pairs = (\n",
    "    com[[\"suggestionId\", \"author\"]].drop_duplicates()\n",
    "       .merge(com[[\"suggestionId\", \"author\"]], on=\"suggestionId\")\n",
    "       .query(\"author_x < author_y\")\n",
    "       .value_counts([\"author_x\", \"author_y\"])\n",
    "       .reset_index(name=\"weight\")\n",
    "       .rename(columns={\"author_x\":\"u\", \"author_y\":\"v\"})\n",
    ")\n",
    "\n",
    "# 2c  User‑Idea bipartite (undirected, unweighted)\n",
    "ui_edges = pd.concat([\n",
    "        sug[[\"author\", \"suggestionId\"]].rename(columns={\"author\":\"user\", \"suggestionId\":\"idea\"}),\n",
    "        com[[\"author\", \"suggestionId\"]].rename(columns={\"author\":\"user\", \"suggestionId\":\"idea\"})\n",
    "    ], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# 2d  Idea‑projection (undirected, weighted)\n",
    "sugg_pairs = (\n",
    "    ui_edges.merge(ui_edges, on=\"user\")\n",
    "            .query(\"idea_x < idea_y\")\n",
    "            .value_counts([\"idea_x\",\"idea_y\"])\n",
    "            .reset_index(name=\"weight\")\n",
    "            .rename(columns={\"idea_x\":\"s1\",\"idea_y\":\"s2\"})\n",
    ")\n",
    "\n",
    "# 2e  **Topic‑specific user–user graphs**  (one per category)\n",
    "topic_edges = {}\n",
    "for cat, ids in sug.groupby(\"category\")[\"suggestionId\"]:\n",
    "    sub = com[com[\"suggestionId\"].isin(ids)][[\"suggestionId\",\"author\"]].drop_duplicates()\n",
    "    t_pairs = (sub.merge(sub, on=\"suggestionId\")\n",
    "                    .query(\"author_x < author_y\")\n",
    "                    .value_counts([\"author_x\",\"author_y\"])\n",
    "                    .reset_index(name=\"weight\"))\n",
    "    topic_edges[cat] = t_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3.  BUILD NetworkX OBJECTS\n",
    "# -----------------------------------------------------------------------------\n",
    "G_flow = nx.DiGraph()\n",
    "G_flow.add_weighted_edges_from(edge_flow[[\"author_com\",\"author_sug\",\"weight\"]].values)\n",
    "\n",
    "G_co = nx.Graph()\n",
    "G_co.add_weighted_edges_from(pairs[[\"u\",\"v\",\"weight\"]].values)\n",
    "\n",
    "G_bip = nx.Graph()\n",
    "G_bip.add_nodes_from(ui_edges[\"user\"].unique(), bipartite=\"user\")\n",
    "G_bip.add_nodes_from(ui_edges[\"idea\"].unique(), bipartite=\"idea\")\n",
    "G_bip.add_edges_from(ui_edges[[\"user\",\"idea\"]].values)\n",
    "\n",
    "G_proj = nx.Graph()\n",
    "G_proj.add_weighted_edges_from(sugg_pairs[[\"s1\",\"s2\",\"weight\"]].values)\n",
    "\n",
    "topic_graphs = {cat: nx.Graph() for cat in topic_edges}\n",
    "for cat, df in topic_edges.items():\n",
    "    topic_graphs[cat].add_weighted_edges_from(df[[\"author_x\",\"author_y\",\"weight\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4.  ROLE TAGGING  (heuristic)   (Q3 pre‑req)\n",
    "# -----------------------------------------------------------------------------\n",
    "activity = com[\"author\"].value_counts()\n",
    "vote_tot = sug.groupby(\"author\")[\"votes\"].sum()\n",
    "act_cut  = activity.quantile(0.90)\n",
    "vote_cut = vote_tot.quantile(0.95)\n",
    "\n",
    "def role(u):\n",
    "    lu = str(u).lower()\n",
    "    if lu.startswith((\"sbx\", \"starbucks_\")):\n",
    "        return \"expert\"\n",
    "    if activity.get(u,0) >= act_cut or vote_tot.get(u,0) >= vote_cut:\n",
    "        return \"contributor\"\n",
    "    return \"client\"\n",
    "\n",
    "roles = {u: role(u) for u in G_co.nodes()}\n",
    "nx.set_node_attributes(G_co, roles, \"role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5.  COMMUNITY DETECTION  (Q2)\n",
    "# -----------------------------------------------------------------------------\n",
    "part_co = community_louvain.best_partition(G_co, weight=\"weight\")\n",
    "nx.set_node_attributes(G_co, part_co, \"community\")\n",
    "\n",
    "# per‑topic communities (optionally saved)\n",
    "topic_comm = {cat: community_louvain.best_partition(g, weight=\"weight\")\n",
    "              for cat, g in topic_graphs.items()}\n",
    "\n",
    "Q_co = nxq.modularity(G_co,\n",
    "                      [{n for n,c in part_co.items() if c==k} for k in set(part_co.values())],\n",
    "                      weight=\"weight\")\n",
    "\n",
    "# intra vs inter traffic\n",
    "intra_w = sum(d[\"weight\"] for u,v,d in G_co.edges(data=True) if part_co[u]==part_co[v])\n",
    "total_w = sum(d[\"weight\"] for _,_,d in G_co.edges(data=True))\n",
    "pct_intra = intra_w / total_w\n",
    "\n",
    "# role assortativity\n",
    "assort = nx.attribute_assortativity_coefficient(G_co,\"role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Ridge Logit AUC (train) : 0.93\n",
      "- Ridge Logit AUC (test)  : 0.90\n",
      "- Best C                  : 0.01\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 7.  RIDGE‑REGULARISED LOGIT with NaN‑safe pipeline  (Q4)\n",
    "# -----------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing   import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute          import SimpleImputer\n",
    "from sklearn.compose         import ColumnTransformer\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.metrics         import roc_auc_score\n",
    "n_comments = com.groupby(\"suggestionId\").size()\n",
    "\n",
    "idea_df = (sug.assign(n_comments = sug[\"suggestionId\"].map(n_comments).fillna(0))\n",
    "               .assign(author_bt = lambda d: d[\"author\"].map(nx.betweenness_centrality(G_co, weight=\"weight\")),\n",
    "                       author_comm = lambda d: d[\"author\"].map(part_co),\n",
    "                       author_role = lambda d: d[\"author\"].map(roles))\n",
    ")\n",
    "\n",
    "# --- success = implemented flag OR top‑decile by votes ---------------------\n",
    "if \"implemented\" in idea_df.columns:\n",
    "    idea_df[\"success\"] = idea_df[\"implemented\"].astype(int)\n",
    "else:\n",
    "    top_dec = idea_df[\"votes\"].quantile(0.90)\n",
    "    idea_df[\"success\"] = (idea_df[\"votes\"] >= top_dec).astype(int)\n",
    "\n",
    "\n",
    "# --- 7.1  Feature engineering  ----------------------------------------------\n",
    "for col in [\"votes\", \"n_comments\"]:\n",
    "    idea_df[col] = pd.to_numeric(idea_df[col], errors=\"coerce\").clip(lower=0).fillna(0)\n",
    "\n",
    "idea_df[\"log_comments\"] = np.log1p(idea_df[\"n_comments\"])\n",
    "idea_df[\"bt_z\"]         = StandardScaler().fit_transform(idea_df[[\"author_bt\"]])\n",
    "\n",
    "idea_df[\"category\"] = idea_df[\"category\"].fillna(\"Unknown\")   # <-- crucial\n",
    "\n",
    "numeric      = [\"log_comments\", \"bt_z\"]\n",
    "categorical  = [\"category\"]\n",
    "target       = idea_df[\"success\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    idea_df[numeric + categorical], target,\n",
    "    test_size=0.30, stratify=target, random_state=42\n",
    ")\n",
    "\n",
    "# --- 7.2  Pre‑processing with imputers  --------------------------------------\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\",  StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", num_pipe, numeric),\n",
    "    (\"cat\", cat_pipe, categorical)\n",
    "])\n",
    "\n",
    "# --- 7.3  Ridge‑logit & grid search  -----------------------------------------\n",
    "logreg = LogisticRegression(\n",
    "    penalty=\"l2\", solver=\"liblinear\", max_iter=500, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"prep\", pre), (\"clf\", logreg)])\n",
    "\n",
    "param_grid = {\"clf__C\": [0.01, 0.1, 1, 10]}\n",
    "cv = GridSearchCV(pipe, param_grid, cv=5, scoring=\"roc_auc\", n_jobs=-1, error_score=\"raise\")\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "bestC      = cv.best_params_[\"clf__C\"]\n",
    "train_auc  = roc_auc_score(y_train, cv.predict_proba(X_train)[:, 1])\n",
    "test_auc   = roc_auc_score(y_test,  cv.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"- Ridge Logit AUC (train) : {train_auc:.2f}\")\n",
    "print(f\"- Ridge Logit AUC (test)  : {test_auc:.2f}\")\n",
    "print(f\"- Best C                  : {bestC}\")\n",
    "\n",
    "with open(\"results/logit_summary.txt\", \"w\") as f:\n",
    "    f.write(\"===== Ridge‑regularised Logit (sklearn) =====\\n\")\n",
    "    f.write(f\"Best C    : {bestC}\\n\")\n",
    "    f.write(f\"Train AUC : {train_auc:.3f}\\n\")\n",
    "    f.write(f\"Test  AUC : {test_auc:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- ensure the output directory exists ---\n",
    "out_dir = os.path.join(\"O3Redo\", \"figs\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# --- your ROC code ---\n",
    "y_score = cv.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(4.5, 4.5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"#888\")\n",
    "plt.xlabel(\"False-Positive Rate\")\n",
    "plt.ylabel(\"True-Positive Rate\")\n",
    "plt.title(\"ROC curve - ridge-logit model\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- save into the newly created directory ---\n",
    "outfile = os.path.join(out_dir, \"roc_curve.png\")\n",
    "plt.savefig(outfile, dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 7.  EXPORTS\n",
    "# -----------------------------------------------------------------------------\n",
    "nx.write_gexf(G_flow,  \"./results/comment_flow.gexf\")\n",
    "nx.write_gexf(G_co,    \"./results/co_commenter.gexf\")\n",
    "nx.write_gexf(G_bip,   \"./results/user_idea_bipartite.gexf\")\n",
    "nx.write_gexf(G_proj,   \"./results/suggestion_projection.gexf\")\n",
    "for cat, g in topic_graphs.items():\n",
    "    nx.write_gexf(g, f\"./results/topics/topic_{cat[:15].replace(' ','_')}.gexf\")\n",
    "\n",
    "idea_df.to_csv(\"./results/idea_features.csv\", index=False)\n",
    "\n",
    "with open(\"./results/ridge_logit_summary.txt\",\"w\") as f:\n",
    "    f.write(f\"Best C      : {bestC}\\n\")\n",
    "    f.write(f\"Train AUC   : {train_auc:.3f}\\n\")\n",
    "    f.write(f\"Test  AUC   : {test_auc:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE COMPLETE  —  KEY NUMBERS\n",
      "Users (nodes, G_co)           : 9100\n",
      "Comment‑flow edges            : 12742\n",
      "Modularity (G_co)             : 0.754\n",
      "Intra‑community traffic       : 83.00%\n",
      "Role assortativity (G_co)     : 0.071\n",
      "Ridge Logit AUC  (train/test) : 0.93 / 0.90\n",
      "Outputs in: /results/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8.  DASHBOARD\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"PIPELINE COMPLETE  —  KEY NUMBERS\")\n",
    "print(f\"Users (nodes, G_co)           : {G_co.number_of_nodes()}\")\n",
    "print(f\"Comment‑flow edges            : {G_flow.number_of_edges()}\")\n",
    "print(f\"Modularity (G_co)             : {Q_co:.3f}\")\n",
    "print(f\"Intra‑community traffic       : {pct_intra:.2%}\")\n",
    "print(f\"Role assortativity (G_co)     : {assort:.3f}\")\n",
    "print(f\"Ridge Logit AUC  (train/test) : {train_auc:.2f} / {test_auc:.2f}\")\n",
    "print(\"Outputs in: /results/\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
