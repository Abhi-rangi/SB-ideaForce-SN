{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import community              as community_louvain     # python‑louvain\n",
    "from networkx.algorithms.community import quality      # modularity\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUG_FILE   = \"combined_500_suggestions.csv\"\n",
    "COM_FILE   =  \"combined_500_comments.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_id = {\"suggestionId\": int, \"commentId\": int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSVs...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CSVs...\")\n",
    "sug = pd.read_csv(SUG_FILE)\n",
    "com = pd.read_csv(COM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building edge lists...\n"
     ]
    }
   ],
   "source": [
    "# ===== 2. Build edge lists =================================================\n",
    "print(\"Building edge lists...\")\n",
    "\n",
    "# 2a. Comment‑flow  (commenter -> suggestion author)\n",
    "edge_flow = (\n",
    "    com.merge(\n",
    "        sug[[\"suggestionId\", \"author\"]],\n",
    "        on=\"suggestionId\",\n",
    "        suffixes=(\"_com\", \"_sug\"),\n",
    "    )\n",
    "    .groupby([\"author_com\", \"author_sug\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"weight\")\n",
    "    .query(\"author_com != author_sug\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Co‑commenter (users who commented on the same suggestion)\n",
    "pairs = (\n",
    "    com[[\"suggestionId\", \"author\"]]\n",
    "    .drop_duplicates()\n",
    "    .merge(com[[\"suggestionId\", \"author\"]], on=\"suggestionId\")\n",
    "    .query(\"author_x < author_y\")                        # remove self & symmetric dupes\n",
    "    .value_counts([\"author_x\", \"author_y\"])\n",
    "    .reset_index(name=\"weight\")\n",
    "    .rename(columns={\"author_x\": \"u\", \"author_y\": \"v\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c. User‑Idea bipartite (author OR commenter edges)\n",
    "ui_edges = pd.concat(\n",
    "    [\n",
    "        sug[[\"author\", \"suggestionId\"]].rename(\n",
    "            columns={\"author\": \"user\", \"suggestionId\": \"idea\"}\n",
    "        ),\n",
    "        com[[\"author\", \"suggestionId\"]].rename(\n",
    "            columns={\"author\": \"user\", \"suggestionId\": \"idea\"}\n",
    "        ),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d. Suggestion projection (ideas linked via shared users)\n",
    "sugg_pairs = (\n",
    "    ui_edges.merge(ui_edges, on=\"user\")\n",
    "    .query(\"idea_x < idea_y\")\n",
    "    .value_counts([\"idea_x\", \"idea_y\"])\n",
    "    .reset_index(name=\"weight\")\n",
    "    .rename(columns={\"idea_x\": \"s1\", \"idea_y\": \"s2\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing NetworkX objects…\n",
      "Tagging roles…\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Build NetworkX graphs\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "print(\"Constructing NetworkX objects…\")\n",
    "\n",
    "G_flow = nx.DiGraph()\n",
    "G_flow.add_weighted_edges_from(\n",
    "    edge_flow[[\"author_com\", \"author_sug\", \"weight\"]].values\n",
    ")\n",
    "\n",
    "G_co = nx.Graph()\n",
    "G_co.add_weighted_edges_from(pairs[[\"u\", \"v\", \"weight\"]].values)\n",
    "\n",
    "G_bip = nx.Graph()\n",
    "G_bip.add_nodes_from(ui_edges[\"user\"].unique(), bipartite=\"user\")\n",
    "G_bip.add_nodes_from(ui_edges[\"idea\"].unique(), bipartite=\"idea\")\n",
    "G_bip.add_edges_from(ui_edges[[\"user\", \"idea\"]].values)\n",
    "\n",
    "G_proj = nx.Graph()\n",
    "G_proj.add_weighted_edges_from(sugg_pairs[[\"s1\", \"s2\", \"weight\"]].values)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4b.  Role tagging  (pure‑heuristic, optional override file)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "print(\"Tagging roles…\")\n",
    "\n",
    "# -- 1. Build per‑user activity metrics ---------------------------------------\n",
    "comment_cnt = com[\"author\"].value_counts()\n",
    "suggest_cnt = sug[\"author\"].value_counts()\n",
    "vote_totals = sug.groupby(\"author\")[\"votes\"].sum()\n",
    "\n",
    "user_metrics = (\n",
    "    pd.DataFrame({\"comments\": comment_cnt,\n",
    "                  \"suggestions\": suggest_cnt,\n",
    "                  \"votes\": vote_totals})\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "# -- 2. Thresholds for “contributor” status -----------------------------------\n",
    "activity_cut = user_metrics[\"comments\"].quantile(0.90)   # top 10 % by comments\n",
    "vote_cut     = user_metrics[\"votes\"].quantile(0.95)      # or top 5 % by total votes\n",
    "\n",
    "def infer_role(user: str, row) -> str:\n",
    "    \"\"\"Heuristic mapping -> expert / contributor / client.\"\"\"\n",
    "    uname = str(user).lower()\n",
    "    if uname.startswith((\"sbx\", \"starbucks_\")):           # employee / expert flag\n",
    "        return \"expert\"\n",
    "    if (row[\"comments\"] >= activity_cut) or (row[\"votes\"] >= vote_cut):\n",
    "        return \"contributor\"\n",
    "    return \"client\"\n",
    "\n",
    "role_dict = {\n",
    "    user: infer_role(user, row) for user, row in user_metrics.iterrows()\n",
    "}\n",
    "\n",
    "# -- 3. Optional override via external CSV ------------------------------------\n",
    "# If someday you receive an authoritative mapping file:\n",
    "# if ROLE_MAP.exists():\n",
    "#     print(\"Override: applying roles from\", ROLE_MAP.name)\n",
    "#     overrides = pd.read_csv(ROLE_MAP)        # columns: user, role\n",
    "#     role_dict.update(dict(zip(overrides[\"user\"], overrides[\"role\"])))\n",
    "\n",
    "# -- 4. Attach roles to graph nodes -------------------------------------------\n",
    "# Any node missing from user_metrics (edge‑case) defaults to 'client'\n",
    "for u in G_co.nodes():\n",
    "    G_co.nodes[u][\"role\"] = role_dict.get(u, \"client\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Louvain…\n",
      "Calculating modularity…\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Community detection & quality\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "print(\"Running Louvain…\")\n",
    "part_co   = community_louvain.best_partition(G_co,   weight=\"weight\")\n",
    "part_proj = community_louvain.best_partition(G_proj, weight=\"weight\")\n",
    "\n",
    "nx.set_node_attributes(G_co,   part_co,   \"community\")\n",
    "nx.set_node_attributes(G_proj, part_proj, \"community\")\n",
    "\n",
    "print(\"Calculating modularity…\")\n",
    "Q_co   = quality.modularity(G_co,   [ {n for n,c in part_co.items()   if c==k} for k in set(part_co.values()) ],   weight=\"weight\")\n",
    "Q_proj = quality.modularity(G_proj, [ {n for n,c in part_proj.items() if c==k} for k in set(part_proj.values()) ], weight=\"weight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating analytics…\n",
      "Computing centralities…\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# 6.  Analytics tables\n",
    "# --------------------------------------------------------------------------- #\n",
    "print(\"Generating analytics…\")\n",
    "\n",
    "# 6a. Community sizes\n",
    "comm_sizes = (\n",
    "    pd.Series(part_co)\n",
    "      .value_counts()\n",
    "      .rename_axis(\"community\")\n",
    "      .reset_index(name=\"size\")\n",
    "      .sort_values(\"community\")\n",
    ")\n",
    "comm_sizes[\"modularity_G_co\"] = Q_co\n",
    "comm_sizes.to_csv(\"gephi_1500/community_sizes_co.csv\", index=False)\n",
    "\n",
    "# 6b. Inter‑community edge weights\n",
    "edges_inter = [\n",
    "    {\"c1\": min(part_co[u], part_co[v]),\n",
    "     \"c2\": max(part_co[u], part_co[v]),\n",
    "     \"weight\": d[\"weight\"]}\n",
    "    for u, v, d in G_co.edges(data=True) if part_co[u] != part_co[v]\n",
    "]\n",
    "pd.DataFrame(edges_inter)\\\n",
    "  .groupby([\"c1\", \"c2\"])[\"weight\"].sum()\\\n",
    "  .reset_index()\\\n",
    "  .to_csv(\"gephi_1500/inter_edges_co.csv\", index=False)\n",
    "\n",
    "# 6c. Role mixing matrix & assortativity\n",
    "roles      = nx.get_node_attributes(G_co, \"role\")\n",
    "role_set   = sorted(set(roles.values()))\n",
    "mix_mtx    = pd.DataFrame(0, index=role_set, columns=role_set, dtype=int)\n",
    "for u, v in G_co.edges():\n",
    "    mix_mtx.loc[roles[u], roles[v]] += 1\n",
    "mix_mtx.to_csv(\"gephi_1500/role_mixing_co.csv\")\n",
    "\n",
    "assort = nx.attribute_assortativity_coefficient(G_co, \"role\")\n",
    "\n",
    "# 6d. Centrality by role\n",
    "print(\"Computing centralities…\")\n",
    "btw  = nx.betweenness_centrality(G_co, weight=\"weight\")\n",
    "deg  = dict(G_co.degree(weight=\"weight\"))\n",
    "\n",
    "cent_rows = []\n",
    "for u in G_co.nodes():\n",
    "    cent_rows.append({\n",
    "        \"user\": u,\n",
    "        \"role\": roles[u],\n",
    "        \"degree_w\": deg[u],\n",
    "        \"betweenness\": btw[u],\n",
    "        \"community\": part_co[u]\n",
    "    })\n",
    "centrality_df = pd.DataFrame(cent_rows)\n",
    "centrality_df.sort_values([\"role\", \"betweenness\"], ascending=[True, False])\\\n",
    "             .to_csv(\"gephi_1500/centrality_by_role.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building idea feature set…\n",
      "- Ridge Logit AUC (train) : 1.00\n",
      "- Ridge Logit AUC (test)  : 1.00\n",
      "- Best C                  : 10\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7.  Idea‑level feature table & regularised modelling  (train/test + separation guard)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "print(\"Building idea feature set…\")\n",
    "\n",
    "# 7.1  Feature assembly --------------------------------------------------------\n",
    "n_comments = com.groupby(\"suggestionId\").size().rename(\"n_comments\")\n",
    "btw = nx.betweenness_centrality(G_co, weight=\"weight\", normalized=True)\n",
    "deg = dict(G_co.degree(weight=\"weight\"))\n",
    "\n",
    "idea_df = (\n",
    "    sug.merge(n_comments, left_on=\"suggestionId\", right_index=True, how=\"left\")\n",
    "       .fillna({\"n_comments\": 0})\n",
    "       .assign(\n",
    "           author_betweenness=lambda d: d[\"author\"].map(lambda u: btw.get(u, 0)),\n",
    "           author_community  =lambda d: d[\"author\"].map(lambda u: part_co.get(u, -1)),\n",
    "           author_role       =lambda d: d[\"author\"].map(lambda u: roles.get(u, \"unknown\")),\n",
    "       )\n",
    ")\n",
    "\n",
    "idea_df[\"success\"] = (idea_df[\"votes\"] >= idea_df[\"votes\"].quantile(0.90)).astype(int)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7.  Ridge‑regularised logistic model (scikit‑learn)\n",
    "# ----------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing   import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose         import ColumnTransformer\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.metrics         import roc_auc_score\n",
    "\n",
    "# numeric pre‑clean\n",
    "for col in [\"votes\", \"n_comments\"]:\n",
    "    idea_df[col] = pd.to_numeric(idea_df[col], errors=\"coerce\").clip(lower=0).fillna(0)\n",
    "\n",
    "idea_df[\"log_votes\"]      = np.log1p(idea_df[\"votes\"])\n",
    "idea_df[\"log_n_comments\"] = np.log1p(idea_df[\"n_comments\"])\n",
    "idea_df[\"author_betweenness_z\"] = StandardScaler().fit_transform(\n",
    "    idea_df[[\"author_betweenness\"]]\n",
    ")\n",
    "\n",
    "target = idea_df[\"success\"]\n",
    "numeric = [\"log_votes\", \"log_n_comments\", \"author_betweenness_z\"]\n",
    "categorical = [\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    idea_df[numeric + categorical], target,\n",
    "    test_size=0.30, stratify=target, random_state=42\n",
    ")\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    [(\"num\", StandardScaler(), numeric),\n",
    "     (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical)]\n",
    ")\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    penalty=\"l2\", solver=\"liblinear\", max_iter=500, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"prep\", pre), (\"clf\", logreg)])\n",
    "\n",
    "param_grid = {\"clf__C\": [0.01, 0.1, 1, 10]}\n",
    "cv = GridSearchCV(pipe, param_grid, cv=5, scoring=\"roc_auc\", n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "best_model = cv.best_estimator_\n",
    "train_auc  = roc_auc_score(y_train, best_model.predict_proba(X_train)[:, 1])\n",
    "test_auc   = roc_auc_score(y_test,  best_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"- Ridge Logit AUC (train) : {train_auc:.2f}\")\n",
    "print(f\"- Ridge Logit AUC (test)  : {test_auc:.2f}\")\n",
    "print(f\"- Best C                  : {cv.best_params_['clf__C']}\")\n",
    "\n",
    "with open(\"gephi_1500/logit_summary.txt\", \"w\") as f:\n",
    "    f.write(\"===== Ridge‑regularised Logit (sklearn) =====\\n\")\n",
    "    f.write(f\"Best C    : {cv.best_params_['clf__C']}\\n\")\n",
    "    f.write(f\"Train AUC : {train_auc:.3f}\\n\")\n",
    "    f.write(f\"Test  AUC : {test_auc:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing GEXF layers…\n",
      "============================================================\n",
      "Pipeline complete.  Key stats\n",
      "- Number of users           : 9100\n",
      "- Number of comments edges  : 12742\n",
      "- Modularity (G_co)         : 0.750\n",
      "- Role assortativity (G_co) : 0.004\n",
      "- Ridge Logit AUC  (train)    : 1.000\n",
      "- Ridge Logit AUC  (test)     : 0.996\n",
      "Outputs saved to:/gephi_1500\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# 8.  Export graphs\n",
    "# --------------------------------------------------------------------------- #\n",
    "print(\"Writing GEXF layers…\")\n",
    "nx.write_gexf(G_flow, \"gephi_1500/comment_flow.gexf\")\n",
    "nx.write_gexf(G_co,   \"gephi_1500/co_commenter.gexf\")\n",
    "nx.write_gexf(G_bip,  \"gephi_1500/user_idea_bipartite.gexf\")\n",
    "nx.write_gexf(G_proj, \"gephi_1500/suggestion_projection.gexf\")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# 9.  Final report\n",
    "# --------------------------------------------------------------------------- #\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline complete.  Key stats\")\n",
    "print(\"- Number of users           :\", G_co.number_of_nodes())\n",
    "print(\"- Number of comments edges  :\", G_flow.number_of_edges())\n",
    "print(f\"- Modularity (G_co)         : {Q_co:.3f}\")\n",
    "print(f\"- Role assortativity (G_co) : {assort:.3f}\")\n",
    "print(f\"- Ridge Logit AUC  (train)    : {train_auc:.3f}\")\n",
    "print(f\"- Ridge Logit AUC  (test)     : {test_auc:.3f}\")\n",
    "print(\"Outputs saved to:\" +\"/gephi_1500\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
